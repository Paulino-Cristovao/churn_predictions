# Churn Prediction for Kolecto Trial Conversions

Predict whether users will convert from trial to paid subscriptions using machine learning. This project implements **5 production-ready models** achieving up to **74% ROC-AUC** (XGBoost) and **78% PR-AUC** (GRU).

## üéØ Quick Start

```bash
# Install dependencies
pip install -r requirements.txt
# Or use poetry
poetry install

# Explore the data
jupyter notebook notebooks/01_data_exploration.ipynb

# Run feature engineering
jupyter notebook notebooks/02_feature_engineering.ipynb

# Train tree-based models
jupyter notebook notebooks/03_tree_models.ipynb

# Train sequential models
python scripts/train_lstm_model.py
python scripts/train_transformer_model.py

# View sequential model analysis
jupyter notebook notebooks/04_sequential_models.ipynb

# Compare all models
python scripts/compare_models.py
```

---

## üìä Model Performance Summary

| Model | Accuracy | ROC-AUC | PR-AUC | Brier | Best For |
|-------|----------|---------|--------|-------|----------|
| **XGBoost** ‚≠ê | **67.5%** | **0.737** | 0.720 | **0.180** | Overall best, production deployment |
| **LightGBM** | **67.5%** | 0.725 | 0.710 | 0.190 | Fast alternative |
| **GRU (Adamax)** | 60.2% | 0.718 | **0.781** ‚≠ê | 0.225 | Best precision-recall, temporal patterns |
| **Transformer** | 62.0% | 0.705 | 0.750 | 0.215 | Advanced sequential modeling |
| Logistic Regression | 60.2% | 0.636 | 0.620 | 0.220 | Baseline, interpretable |

**Improvement from baseline**: +14% accuracy, +12% ROC-AUC

---

## üîë Key Insights

### Top 5 Conversion Drivers (SHAP Analysis)
1. **Late trial activity** (days 12-14) ‚Üí 3x higher conversion
2. **Feature diversity** (5+ features) ‚Üí 2.5x boost
3. **Early engagement** (days 0-2) ‚Üí 2x higher rate
4. **Invoice creation** by day 7 ‚Üí 85% conversion
5. **Banking connections** ‚Üí Strong commitment signal

### Business Recommendations
- **CS Team**: Proactive outreach for users with <3 features by day 7
- **Product**: Drive 5+ feature adoption in first 3 days
- **Marketing**: Focus on TPE/PME segments (65% vs 52% conversion)

**Expected Impact**: +4-6% conversion improvement (60.7% ‚Üí 66%)

---

## üìÅ Project Structure

```
churn_predictions/
‚îú‚îÄ‚îÄ data/                          # Data files
‚îÇ   ‚îî‚îÄ‚îÄ raw/                       # Original datasets
‚îÇ       ‚îú‚îÄ‚îÄ daily_usage.csv
‚îÇ       ‚îú‚îÄ‚îÄ subscriptions.csv
‚îÇ       ‚îú‚îÄ‚îÄ Case Study Data Scientist.pdf
‚îÇ       ‚îî‚îÄ‚îÄ README.md             # Data dictionary
‚îÇ
‚îú‚îÄ‚îÄ notebooks/                     # Jupyter notebooks
‚îÇ   ‚îú‚îÄ‚îÄ 01_data_exploration.ipynb  # Initial data analysis
‚îÇ   ‚îú‚îÄ‚îÄ 02_feature_engineering.ipynb  # Feature creation
‚îÇ   ‚îú‚îÄ‚îÄ 03_tree_models.ipynb      # Tree-based models (LR, XGBoost, LightGBM)
‚îÇ   ‚îú‚îÄ‚îÄ 04_sequential_models.ipynb # Sequential model analysis
‚îÇ   ‚îî‚îÄ‚îÄ churn_analysis_original.ipynb  # Original unified notebook (backup)
‚îÇ
‚îú‚îÄ‚îÄ src/                           # Source code
‚îÇ   ‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocessing.py      # Data loading and preprocessing
‚îÇ   ‚îú‚îÄ‚îÄ features/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ engineering.py        # Feature engineering functions
‚îÇ   ‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm/                 # GRU/LSTM implementation
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model.py
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ train.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformer/          # Transformer implementation
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ model.py
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py
‚îÇ       ‚îî‚îÄ‚îÄ validation.py         # Pydantic data models
‚îÇ
‚îú‚îÄ‚îÄ scripts/                       # Training and evaluation scripts
‚îÇ   ‚îú‚îÄ‚îÄ train_lstm_model.py       # Train GRU model
‚îÇ   ‚îú‚îÄ‚îÄ train_transformer_model.py # Train Transformer model
‚îÇ   ‚îî‚îÄ‚îÄ compare_models.py         # Compare all models
‚îÇ
‚îú‚îÄ‚îÄ results/                       # Outputs and artifacts
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Saved model weights
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm_best_model.pt
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformer_best_model.pt
‚îÇ   ‚îú‚îÄ‚îÄ figures/                  # Visualizations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ feature_importance.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ roc_curves.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ model_comparison.png
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lstm_training_curves.png
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transformer_training_curves.png
‚îÇ   ‚îî‚îÄ‚îÄ metrics/                  # Model performance metrics
‚îÇ       ‚îú‚îÄ‚îÄ lstm_results.json
‚îÇ       ‚îî‚îÄ‚îÄ transformer_results.json
‚îÇ
‚îú‚îÄ‚îÄ docs/                          # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ MODEL_ANALYSIS_REPORT.md  # ‚≠ê Comprehensive analysis
‚îÇ   ‚îú‚îÄ‚îÄ SUMMARY.md                # Complete summary
‚îÇ   ‚îú‚îÄ‚îÄ executive_presentation.md # Business stakeholder deck
‚îÇ   ‚îú‚îÄ‚îÄ next_steps.md            # Deployment roadmap
‚îÇ   ‚îî‚îÄ‚îÄ TRAINING_TIME_ANALYSIS.md # Training performance
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt              # Python dependencies
‚îú‚îÄ‚îÄ pyproject.toml               # Poetry configuration
‚îî‚îÄ‚îÄ README.md                    # This file
```

---

## üöÄ Getting Started

### Prerequisites
- Python 3.8+
- PyTorch 2.0+ (for sequential models)
- Jupyter Notebook

### Installation

```bash
# Clone the repository
git clone https://github.com/Paulino-Cristovao/churn_predictions.git
cd churn_predictions

# Install dependencies
pip install -r requirements.txt

# Or use poetry
poetry install
```

### Running Notebooks

The project is organized into 4 sequential notebooks:

1. **Data Exploration** (`01_data_exploration.ipynb`)
   - Load and validate data
   - Explore conversion patterns
   - Identify data quality issues

2. **Feature Engineering** (`02_feature_engineering.ipynb`)
   - Create temporal features
   - Build milestone and diversity metrics
   - Analyze feature correlations

3. **Tree Models** (`03_tree_models.ipynb`)
   - Train Logistic Regression, XGBoost, LightGBM
   - SHAP analysis and interpretability
   - Model comparison

4. **Sequential Models** (`04_sequential_models.ipynb`)
   - Analyze LSTM/GRU and Transformer results
   - Compare with tree-based models
   - Business recommendations

### Training Models

```bash
# Train GRU model (100 epochs, ~10-15 minutes)
python scripts/train_lstm_model.py

# Train Transformer model (100 epochs, ~15-20 minutes)
python scripts/train_transformer_model.py

# Compare all models
python scripts/compare_models.py
```

---

## üìö Documentation

| Document | Description |
|----------|-------------|
| [MODEL_ANALYSIS_REPORT.md](docs/MODEL_ANALYSIS_REPORT.md) | **‚≠ê Comprehensive model comparison and recommendations** |
| [SUMMARY.md](docs/SUMMARY.md) | Complete project summary with all results |
| [executive_presentation.md](docs/executive_presentation.md) | Business stakeholder presentation |
| [next_steps.md](docs/next_steps.md) | Deployment roadmap and future work |
| [data/raw/README.md](data/raw/README.md) | Data dictionary and descriptions |

---

## üî¨ Technical Highlights

### Data Processing
- **Preprocessing**: Date handling, filtering, target definition (see `src/data/preprocessing.py`)
- **Feature Engineering**: 90+ features including temporal, milestone, diversity, and velocity metrics (see `src/features/engineering.py`)
- **Validation**: Pydantic models for data integrity (see `src/utils/validation.py`)

### Models
- **Tree-Based**: Logistic Regression, XGBoost, LightGBM with hyperparameter tuning
- **Sequential**: GRU with Adamax optimizer, Transformer with attention
- **Evaluation**: ROC-AUC, PR-AUC, Brier Score, calibration analysis

### Interpretability
- SHAP values for feature importance
- Temporal pattern analysis
- Conversion driver identification

---

## üéØ Next Steps

1. ‚úÖ **Complete** - All 5 models trained and documented
2. ‚úÖ **Complete** - Project reorganized with clean structure
3. ‚è≠Ô∏è Deploy XGBoost to production API
4. ‚è≠Ô∏è CRM integration for CS team scoring
5. ‚è≠Ô∏è A/B test interventions
6. ‚è≠Ô∏è Monthly model retraining pipeline

---

## üìà Results

The complete model comparison is available in `results/figures/model_comparison.png`.

Key findings:
- **XGBoost** achieves the best overall performance with 73.7% ROC-AUC
- **GRU** excels at precision-recall with 78.1% PR-AUC
- Sequential models better capture temporal patterns
- Tree-based models offer faster inference and easier interpretation

---

**Project Status**: ‚úÖ Production Ready  
**Best Model**: XGBoost (73.7% ROC-AUC)  
**Best PR-AUC**: GRU (78.1%)  
**Last Updated**: December 8, 2025

**GitHub**: https://github.com/Paulino-Cristovao/churn_predictions

---

## üìù License

This project is part of a data science case study for Kolecto.

## üë§ Author

**Paulino Cristovao**
