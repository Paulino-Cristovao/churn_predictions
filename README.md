# Kolecto Churn Prediction System "Pro"

![Status](https://img.shields.io/badge/Status-Production_Ready-success)
![Python](https://img.shields.io/badge/Python-3.10+-blue)
![Pytorch](https://img.shields.io/badge/PyTorch-2.0+-orange)
![Gradio](https://img.shields.io/badge/Gradio-App-pink)

> **Predicting B2B Customer Churn with Deep Learning and 157-Dimensional Feature Engineering.**

---

## Executive Summary
This project verifies and implements a robust, end-to-end machine learning solution to predict customer churn for **Kolecto**, based on a case study of SaaS trial conversions. By analyzing **~11,600 daily usage records** and **~500 subscriptions** (filtered to **416 exact 15-day trials** for consistency), the system identifies significantly predictive signals of conversion (**~60% conversion rate**) versus cancellation.

**Key Highlights:**
*   **State-of-the-Art Models**: Compare **LightGBM**, **XGBoost**, **Logistic Regression**, **GRU (RNN)**, and **Transformer**.
| Model | ROC-AUC | PR-AUC | Notes |
| :--- | :--- | :--- | :--- |
| **LightGBM + Optuna** | 0.7976 | 0.8414 | **Best Single Model** (Fast & Accurate) |

### ðŸ† Pourquoi LightGBM & l'Ensemble gagnent ?
LightGBM gÃ¨re mieux les donnÃ©es tabulaires denses que les rÃ©seaux de neurones sur ce petit volume de donnÃ©es. Il capture efficacement les interactions non-linÃ©aires sans nÃ©cessiter des milliers d'exemples comme le Deep Learning.
*   **Deep Feature Engineering**: Automates the processing of **157 features**, aggregating **19 daily usage metrics** (e.g., `nb_transfers_sent`, `nb_mobile_connections`) via sum/mean/max/std, combined with categorical firmographics like *NAF Codes* and *Revenue Ranges*.
*   **Interactive Application**: A user-friendly Gradio web interface (`app.py`) for real-time scoring.
*   **Top Performance**: **LightGBM** achieves **AUC ~0.80**, capturing non-linear interactions better than baselines.

---

## Data Sources & Structure
The analysis is based on two primary datasets provided in the case study:

*   **`subscriptions.csv`**: Static information on trials.
    *   **Size**: ~503 raw rows, filtered to 416 for analysis.
    *   **Key Fields**: `subscription_id`, `trial_starts_at` (2023-2024 range), `v2_segment` (e.g., "TPE/PME"), `naf_section`, `revenue_range`.
    *   **Target**: Derived from `first_paid_invoice_paid_at` (1 = Converted, 0 = Churn).
*   **`daily_usage.csv`**: Time-series activity log.
    *   **Size**: ~11,685 rows.
    *   **Key Fields**: `day_date`, and 19+ activity counters (e.g., `nb_client_invoices_created`, `nb_banking_accounts_connected`).
    *   **Processing**: Aggregated to single-row features per subscription to capture total engagement and variability.

---

## Model Performance Leaderboard

| Model | Accuracy | ROC-AUC | PR-AUC | Calibration (Brier) |
| :--- | :--- | :--- | :--- | :--- |
| **LightGBM** | **71.1%** | **0.798** | **0.841** | **0.194** |
| **GRU (RNN)** | 68.7% | 0.715 | 0.764 | 0.213 |
| **Logistic Regression** | 65.1% | 0.684 | 0.769 | 0.229 |
| **Transformer** | 62.7% | 0.678 | 0.706 | 0.221 |
| **XGBoost** | 63.9% | 0.671 | 0.772 | 0.242 |

> *See [docs/METRICS_EXPLANATION.md](docs/METRICS_EXPLANATION.md) for details on these metrics.*

---

## Architecture

The system is modular, ensuring reproducibility and easy maintenance.

```mermaid
graph LR
    A[Raw Data] --> B(01_Data_Processing)
    B --> C{Feature Engineering}
    C -->|OneHot + Ordinal| D[X_train (157 feats)]
    D --> E[Model Requests]
    E --> F[LightGBM]
    E --> G[GRU / Transformer]
    E --> H[XGBoost]
    F & G & H --> I[Verification & Selection]
    I --> J[Gradio App]
```

---

## Installation & Usage

### 1. Setup Environment
Ensure you have Python 3.10+ installed.
Using pip:
```bash
pip install -r requirements.txt
```

Using poetry:
```bash
poetry install
```

### 2. Run the Interactive App
Launch the web interface to test predictions on new data.
```bash
python app.py
```
*The app will open in your browser at `http://0.0.0.0:7860`.*

### 3. Reproduce Training
To re-run the entire pipeline from scratch:
```bash
# Process Data
jupyter nbconvert --to notebook --execute notebooks/01_data_processing.ipynb --inplace

# Train Models (e.g., LightGBM)
jupyter nbconvert --to notebook --execute notebooks/04_lightgbm.ipynb --inplace
```

---

## Documentation Index

All detailed documentation is located in the `docs/` directory:

*   **[Model Documentation](docs/MODEL_DOCUMENTATION.md)**: Deep dive into architectures, hyperparameters, and training strategies.
*   **[Metrics Explanation](docs/METRICS_EXPLANATION.md)**: Guide to understanding AUC, Precision, Recall, etc.
*   **[Next Steps](docs/NEXT_STEPS.md)**: Roadmap for future improvements (Deployment, MLOps).
*   **[Testing Results](docs/TESTING_RESULTS.md)**: Logs of validation runs.

---

## Repository Structure

```
â”œâ”€â”€ app.py                  # Main Gradio Application
â”œâ”€â”€ notebooks/              # Jupyter Notebooks (Data, Training, Evaluation)
â”‚   â”œâ”€â”€ 01_data_processing.ipynb
â”‚   â”œâ”€â”€ ...
â”‚   â””â”€â”€ 07_model_comparison.ipynb
â”œâ”€â”€ models/                 # PyTorch Model Definitions (GRU/Transformer)
â”œâ”€â”€ config/                 # Configuration Files
â”œâ”€â”€ docs/                   # Documentation
â”œâ”€â”€ results/                # Saved Artifacts (Models, Figures, Metrics)
â”‚   â”œâ”€â”€ models/             # PKL and PT files
â”‚   â”œâ”€â”€ preprocessor.pkl    # Fitted ColumnTransformer
â”‚   â”œâ”€â”€ app_config.json     # App Dropdown Options
â”‚   â””â”€â”€ feature_columns.json # Feature Mapping
â”œâ”€â”€ requirements.txt        # Dependencies
â””â”€â”€ poetry.lock             # Poetry Lockfile
```

---

*Built for Kolecto by "Antigravity"*
