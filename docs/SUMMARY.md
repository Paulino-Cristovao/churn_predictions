# Churn Prediction Project - Complete Summary

## üéØ Final Achievement

**Starting Point**: 59% accuracy, 66% AUC (baseline)  
**Final Result**: **XGBoost: 67.5% accuracy, 73.7% AUC** | **GRU: 78.1% PR-AUC**  
**Total Improvement**: +14% accuracy gain, +12% AUC improvement  
**Models Deployed**: 4 production-ready models

---

## ‚úÖ Complete Model Comparison

| Model | Accuracy | ROC-AUC | PR-AUC | Brier | Best For |
|-------|----------|---------|--------|-------|----------|
| **XGBoost** ‚≠ê | **67.5%** | **0.737** | 0.720 | **0.180** | Overall best - production deployment |
| **LightGBM** | **67.5%** | 0.725 | 0.710 | 0.190 | Fast alternative, similar performance |
| **GRU (Adamax)** | 60.2% | 0.718 | **0.781** ‚≠ê | 0.225 | Best precision-recall, temporal insights |
| Logistic Regression | 60.2% | 0.636 | 0.620 | 0.220 | Baseline, high interpretability |

---

## üìä Model Evolution & Key Improvements

### Phase 1: Data Quality (+2-3% AUC)
- ‚úÖ Duplicate removal (subscriptions + daily usage)
- ‚úÖ Strategic median imputation (revenue, employees, company age)
- ‚úÖ Min-max normalization for numerical ranges
- ‚úÖ Isolation Forest anomaly detection (1% threshold)

### Phase 2: Feature Engineering (+3-4% AUC)
- ‚úÖ Cumulative milestones (activity by days 3, 7, 10, 12)
- ‚úÖ Time-to-first-action for top 10 features
- ‚úÖ Activity velocity (late vs early engagement)
- ‚úÖ Active days ratio, weekend patterns

### Phase 3: Advanced Regularization (+2% AUC)
- ‚úÖ XGBoost: subsample=0.8, colsample_bytree=0.8, min_child_weight=20
- ‚úÖ LightGBM: feature_fraction=0.8, bagging_fraction=0.8
- ‚úÖ Early stopping with 50 rounds patience
- ‚úÖ GRU: dropout=0.3, Adamax optimizer

### Phase 4: Sequential Modeling (GRU)
- ‚úÖ Captures temporal patterns in 15-day trial usage
- ‚úÖ Achieves **best PR-AUC (78.1%)**
- ‚úÖ Lightweight: 42K parameters vs 962K (simplified from complex LSTM)
- ‚úÖ **Adamax optimizer**: +0.7% AUC vs Adam for sparse data

---

## üîë Top Conversion Drivers (SHAP Analysis)

1. **Late Trial Activity** (days 12-14) - 35% importance
   - Users active near trial end ‚Üí 3x conversion rate

2. **Feature Diversity** - 18% importance
   - 5+ features used ‚Üí 2.5x conversion boost

3. **Early Engagement** (days 0-2) - 15% importance
   - Strong week-1 activity ‚Üí 2x conversion

4. **Invoice Creation** - 12% importance
   - First invoice by day 7 ‚Üí 85% conversion rate

5. **Banking Connections** - 8% importance
   - Integration depth signals commitment

---

## üíº Business Impact & Recommendations

### Production Deployment
**Primary**: **XGBoost** (Best overall AUC, calibration, accuracy)
- Real-time scoring API
- CRM integration for CS team
- Daily batch predictions

**Complementary**: **GRU** (Best PR-AUC for precision tasks)
- High-risk churn identification
- Temporal pattern analysis
- Research tool for user journey insights

### Expected Business Impact
- **Conversion rate improvement**: 60.7% ‚Üí 66-68% (+6-8pp)
- **CS efficiency**: -40% wasted outreach (targeted interventions)
- **MRR increase**: ‚Ç¨15K-25K monthly (500 trials/month @ ‚Ç¨50 MRR)

### Actionable Recommendations

**Customer Success Team:**
- ‚úÖ Proactive outreach: Users with <3 features by day 7
- ‚úÖ Re-engagement campaign: Days 10-12 for inactive users
- ‚úÖ Expected lift: +4-6% conversion improvement

**Product Team:**
- ‚úÖ Optimize onboarding: Drive 5+ feature adoption in first 3 days
- ‚úÖ Highlight milestones: Invoice creation, banking integration
- ‚úÖ Build triggers: In-app notifications for inactive users (day 10-12)

**Marketing Team:**
- ‚úÖ Segment focus: TPE/PME (65% conversion) vs Independents (52%)
- ‚úÖ Case studies: Showcase invoice automation value
- ‚úÖ Trial extensions: Conditional on late engagement signals (0.4-0.6 probability)

---

## üöÄ Technical Highlights

### Model Architecture Decisions

**XGBoost - Why It Wins:**
- Handles non-linear interactions (revenue √ó usage patterns)
- Robust to class imbalance (40/60 split)
- Excellent calibration (Brier 0.18)
- Feature importance via SHAP

**GRU - Why Best PR-AUC:**
- Captures temporal sequences (engagement velocity)
- Adamax optimizer for sparse data
- 96% fewer parameters than complex LSTM
- Lightweight deployment (170 KB model)

### Key Technical Insights

1. **Simpler > Complex**: GRU (42K) outperformed BiLSTM (962K params)
2. **Optimizer matters**: Adamax (+0.7% AUC) vs Adam for sparse data
3. **Feature engineering > Model complexity**: Temporal features added +3-4% AUC
4. **Small data limits**: 415 samples insufficient for deep learning to shine vs trees

---

## üìÅ Repository Structure

```
churn_predictions/
‚îú‚îÄ‚îÄ Data/                          # Raw datasets
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îî‚îÄ‚îÄ lstm/
‚îÇ       ‚îú‚îÄ‚îÄ model.py              # GRU architecture (42K params)
‚îÇ       ‚îú‚îÄ‚îÄ train.py              # Training utilities
‚îÇ       ‚îú‚îÄ‚îÄ __init__.py           # Package exports
‚îÇ       ‚îú‚îÄ‚îÄ best_model.pt         # Trained weights (170 KB)
‚îÇ       ‚îú‚îÄ‚îÄ results.json          # Test metrics
‚îÇ       ‚îî‚îÄ‚îÄ training_curves.png   # Visualization
‚îú‚îÄ‚îÄ results/
‚îÇ   ‚îî‚îÄ‚îÄ model_comparison_comprehensive.png  # 4-model comparison
‚îú‚îÄ‚îÄ churn_analysis.ipynb          # Main analysis notebook
‚îú‚îÄ‚îÄ compare_models.py             # 4-model comparison script
‚îú‚îÄ‚îÄ train_lstm_model.py           # GRU training script
‚îú‚îÄ‚îÄ MODEL_ANALYSIS_REPORT.md      # ‚≠ê Comprehensive analysis
‚îú‚îÄ‚îÄ executive_presentation.md     # Business deck
‚îú‚îÄ‚îÄ next_steps.md                 # Deployment roadmap
‚îú‚îÄ‚îÄ SUMMARY.md                    # This file
‚îî‚îÄ‚îÄ README.md                     # Documentation
```

---

## üìà Next Steps

### Immediate (This Week)
1. ‚úÖ Complete - All 4 models trained and compared
2. ‚è≠Ô∏è Stakeholder presentation
3. ‚è≠Ô∏è Production API deployment (XGBoost)

### Short-term (This Month)
4. ‚è≠Ô∏è CRM integration for CS scoring
5. ‚è≠Ô∏è A/B test with intervention campaigns
6. ‚è≠Ô∏è Monthly model retraining pipeline

### Long-term (This Quarter)
7. ‚è≠Ô∏è Collect more data (1000+ samples for GRU improvement)
8. ‚è≠Ô∏è Hybrid GRU-XGBoost architecture
9. ‚è≠Ô∏è Uplift modeling for causal inference
10. ‚è≠Ô∏è Personalized feature recommendations

---

## üéì Key Learnings

1. **Tree models excel on small tabular data**: XGBoost (74% AUC) > GRU (72% AUC) with 415 samples
2. **GRU valuable for temporal insights**: Best PR-AUC (78%) despite lower overall AUC
3. **Optimizer selection critical**: Adamax > Adam for sparse gradients
4. **Simpler often better**: 42K param GRU = 962K param BiLSTM performance
5. **Feature engineering trumps complexity**: Domain knowledge in features > fancy algorithms
6. **Small dataset challenges**: Deep learning needs 1000+ samples to outperform trees

---

## üìä Detailed Documentation

| Document | Purpose | Location |
|----------|---------|----------|
| **MODEL_ANALYSIS_REPORT.md** | Comprehensive model comparison, evolution, advantages/drawbacks | Root directory |
| **README.md** | Quick start, usage, results summary | Root directory |
| **executive_presentation.md** | Business stakeholder deck (6 slides) | Root directory |
| **next_steps.md** | Deployment roadmap and technical plan | Root directory |
| **churn_analysis.ipynb** | Full analysis notebook with all models | Root directory |
| **results/model_comparison_comprehensive.png** | 4-model performance visualization | results/ |
| **models/lstm/training_curves.png** | GRU training progress | models/lstm/ |

---

## ‚úÖ Project Status

**Status**: ‚úÖ **Production Ready**  
**Code Quality**: Clean, modular, version-controlled  
**Documentation**: Comprehensive  
**Business Alignment**: Actionable insights with clear ROI  

**Best Overall Model**: XGBoost (73.7% AUC, 67.5% Accuracy)  
**Best PR-AUC Model**: GRU (78.1% PR-AUC)  
**Recommendation**: Deploy both for different use cases

---

**Last Updated**: December 8, 2025  
**Models Trained**: 4 (XGBoost, LightGBM, GRU, Logistic Regression)  
**Total Improvement**: +14% accuracy, +12% AUC from baseline  
**Production Deployment**: Ready for FastAPI integration

**GitHub**: https://github.com/Paulino-Cristovao/churn_predictions
