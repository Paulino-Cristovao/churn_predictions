{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542301e1",
   "metadata": {},
   "source": [
    "# Kolecto Churn Prediction - Complete Analysis\n",
    "\n",
    "**Objective**: Predict conversion from 15-day trial to paid subscription\n",
    "\n",
    "**Models Implemented**:\n",
    "1. Logistic Regression (baseline)\n",
    "2. XGBoost (gradient boosting)\n",
    "3. LightGBM (fast gradient boosting)\n",
    "4. LSTM/GRU (sequential model)\n",
    "5. Transformer (attention-based)\n",
    "\n",
    "**Data**: \n",
    "- 503 trials (filtered to 15-day duration)\n",
    "- ~60% baseline conversion rate\n",
    "- 20 daily usage features\n",
    "\n",
    "All visualizations saved to `../results/figures/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb51025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking dependencies...\n",
      "âœ… pandas\n",
      "âœ… numpy\n",
      "âœ… scikit-learn\n",
      "âœ… xgboost\n",
      "âœ… lightgbm\n",
      "âŒ torch - MISSING\n",
      "âœ… matplotlib\n",
      "âœ… seaborn\n",
      "\n",
      "âš ï¸  Installing missing packages: torch\n"
     ]
    }
   ],
   "source": [
    "# Verify all dependencies are installed\n",
    "import sys\n",
    "\n",
    "print(\"Checking dependencies...\")\n",
    "required_packages = {\n",
    "    'pandas': 'pandas',\n",
    "    'numpy': 'numpy',\n",
    "    'sklearn': 'scikit-learn',\n",
    "    'xgboost': 'xgboost',\n",
    "    'lightgbm': 'lightgbm',\n",
    "    'torch': 'torch',\n",
    "    'matplotlib': 'matplotlib',\n",
    "    'seaborn': 'seaborn'\n",
    "}\n",
    "\n",
    "missing = []\n",
    "for module, package in required_packages.items():\n",
    "    try:\n",
    "        __import__(module)\n",
    "        print(f\"âœ… {package}\")\n",
    "    except ImportError:\n",
    "        print(f\"âŒ {package} - MISSING\")\n",
    "        missing.append(package)\n",
    "\n",
    "if missing:\n",
    "    print(f\"\\nâš ï¸  Installing missing packages: {', '.join(missing)}\")\n",
    "    for pkg in missing:\n",
    "        !{sys.executable} -m pip install {pkg} --quiet\n",
    "    print(\"âœ… All packages installed!\")\n",
    "else:\n",
    "    print(\"\\nâœ… All dependencies verified!\")\n",
    "    \n",
    "# Verify PyTorch specifically\n",
    "import torch\n",
    "print(f\"\\nðŸ”¥ PyTorch version: {torch.__version__}\")\n",
    "print(f\"   Device: {'cuda' if torch.cuda.is_available() else 'cpu'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630d16a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4560784",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mshap\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# Deep learning\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnn\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DataLoader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, roc_curve, \n",
    "    precision_recall_curve, auc, brier_score_loss,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Tree models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"âœ… All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43c226",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data from ../data/raw/...\")\n",
    "subscriptions = pd.read_csv('../data/raw/subscriptions.csv')\n",
    "daily_usage = pd.read_csv('../data/raw/daily_usage.csv')\n",
    "\n",
    "print(f\"Subscriptions: {subscriptions.shape}\")\n",
    "print(f\"Daily usage: {daily_usage.shape}\")\n",
    "\n",
    "# Convert dates\n",
    "date_cols = ['trial_starts_at', 'trial_ends_at', 'first_paid_invoice_paid_at']\n",
    "for col in date_cols:\n",
    "    subscriptions[col] = pd.to_datetime(subscriptions[col], errors='coerce')\n",
    "\n",
    "# Calculate trial duration\n",
    "subscriptions['trial_duration'] = (\n",
    "    subscriptions['trial_ends_at'] - subscriptions['trial_starts_at']\n",
    ").dt.days\n",
    "\n",
    "# Filter to 15-day trials only (as per case study)\n",
    "print(f\"\\nTrial duration distribution:\\n{subscriptions['trial_duration'].value_counts()}\")\n",
    "subscriptions_15d = subscriptions[subscriptions['trial_duration'] == 15].copy()\n",
    "print(f\"\\nAfter filtering to 15-day trials: {len(subscriptions_15d)} trials\")\n",
    "\n",
    "# Define target: converted if they have a paid invoice\n",
    "subscriptions_15d['converted'] = subscriptions_15d['first_paid_invoice_paid_at'].notna().astype(int)\n",
    "conversion_rate = subscriptions_15d['converted'].mean()\n",
    "print(f\"\\nâœ… Conversion rate: {conversion_rate:.2%}\")\n",
    "print(f\"   Converted: {subscriptions_15d['converted'].sum()}\")\n",
    "print(f\"   Not converted: {(~subscriptions_15d['converted'].astype(bool)).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59910b",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate usage features per trial\n",
    "usage_cols = [col for col in daily_usage.columns if col.startswith('nb_')]\n",
    "print(f\"Found {len(usage_cols)} usage features\")\n",
    "\n",
    "# Aggregate: sum, mean, max, std for each trial\n",
    "usage_agg = daily_usage.groupby('subscription_id')[usage_cols].agg(\n",
    "    ['sum', 'mean', 'max', 'std']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "usage_agg.columns = ['subscription_id'] + [\n",
    "    f'{col[0]}_{col[1]}' for col in usage_agg.columns[1:]\n",
    "]\n",
    "\n",
    "# Fill NaN std with 0\n",
    "usage_agg = usage_agg.fillna(0)\n",
    "\n",
    "print(f\"Aggregated usage features: {usage_agg.shape}\")\n",
    "\n",
    "# Merge with subscriptions\n",
    "df = subscriptions_15d.merge(usage_agg, on='subscription_id', how='left')\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"\\nâœ… Final dataset: {df.shape}\")\n",
    "print(f\"   Features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb54c55",
   "metadata": {},
   "source": [
    "### 2.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# Use only numerical usage features\n",
    "feature_cols = [col for col in df.columns if col.startswith('nb_')]\n",
    "X = df[feature_cols].values\n",
    "y = df['converted'].values\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(y)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train conversion: {y_train.mean():.2%}\")\n",
    "print(f\"Test conversion: {y_test.mean():.2%}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\nâœ… Data prepared for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a78845",
   "metadata": {},
   "source": [
    "## 3. Model 1: Logistic Regression (Baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34127612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Logistic Regression\n",
    "print(\"Training Logistic Regression...\")\n",
    "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions\n",
    "lr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "lr_pred = (lr_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "lr_accuracy = accuracy_score(y_test, lr_pred)\n",
    "lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "\n",
    "lr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_pred_proba)\n",
    "lr_pr_auc = auc(lr_recall, lr_precision)\n",
    "\n",
    "lr_brier = brier_score_loss(y_test, lr_pred_proba)\n",
    "\n",
    "# Store results\n",
    "lr_results = {\n",
    "    'accuracy': lr_accuracy,\n",
    "    'roc_auc': lr_auc,\n",
    "    'pr_auc': lr_pr_auc,\n",
    "    'brier': lr_brier\n",
    "}\n",
    "\n",
    "print(f\"âœ… Logistic Regression Results:\")\n",
    "print(f\"   Accuracy: {lr_accuracy:.3f}\")\n",
    "print(f\"   ROC-AUC: {lr_auc:.3f}\")\n",
    "print(f\"   PR-AUC: {lr_pr_auc:.3f}\")\n",
    "print(f\"   Brier Score: {lr_brier:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dca5168",
   "metadata": {},
   "source": [
    "## 4. Model 2: XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6909503d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost\n",
    "print(\"Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42,\n",
    "    eval_metric='logloss'\n",
    ")\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "xgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\n",
    "xgb_pred = (xgb_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "xgb_accuracy = accuracy_score(y_test, xgb_pred)\n",
    "xgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n",
    "\n",
    "xgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_pred_proba)\n",
    "xgb_pr_auc = auc(xgb_recall, xgb_precision)\n",
    "\n",
    "xgb_brier = brier_score_loss(y_test, xgb_pred_proba)\n",
    "\n",
    "# Store results\n",
    "xgb_results = {\n",
    "    'accuracy': xgb_accuracy,\n",
    "    'roc_auc': xgb_auc,\n",
    "    'pr_auc': xgb_pr_auc,\n",
    "    'brier': xgb_brier\n",
    "}\n",
    "\n",
    "print(f\"âœ… XGBoost Results:\")\n",
    "print(f\"   Accuracy: {xgb_accuracy:.3f}\")\n",
    "print(f\"   ROC-AUC: {xgb_auc:.3f}\")\n",
    "print(f\"   PR-AUC: {xgb_pr_auc:.3f}\")\n",
    "print(f\"   Brier Score: {xgb_brier:.3f}\")\n",
    "\n",
    "# Feature importance plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "xgb.plot_importance(xgb_model, ax=ax, max_num_features=15)\n",
    "plt.title('XGBoost Feature Importance (Top 15)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/xgb_feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… Saved: ../results/figures/xgb_feature_importance.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9fae0f",
   "metadata": {},
   "source": [
    "## 5. Model 3: LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462edc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM\n",
    "print(\"Training LightGBM...\")\n",
    "lgb_model = lgb.LGBMClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "lgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "lgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\n",
    "lgb_pred = (lgb_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "# Metrics\n",
    "lgb_accuracy = accuracy_score(y_test, lgb_pred)\n",
    "lgb_auc = roc_auc_score(y_test, lgb_pred_proba)\n",
    "\n",
    "lgb_precision, lgb_recall, _ = precision_recall_curve(y_test, lgb_pred_proba)\n",
    "lgb_pr_auc = auc(lgb_recall, lgb_precision)\n",
    "\n",
    "lgb_brier = brier_score_loss(y_test, lgb_pred_proba)\n",
    "\n",
    "# Store results\n",
    "lgb_results = {\n",
    "    'accuracy': lgb_accuracy,\n",
    "    'roc_auc': lgb_auc,\n",
    "    'pr_auc': lgb_pr_auc,\n",
    "    'brier': lgb_brier\n",
    "}\n",
    "\n",
    "print(f\"âœ… LightGBM Results:\")\n",
    "print(f\"   Accuracy: {lgb_accuracy:.3f}\")\n",
    "print(f\"   ROC-AUC: {lgb_auc:.3f}\")\n",
    "print(f\"   PR-AUC: {lgb_pr_auc:.3f}\")\n",
    "print(f\"   Brier Score: {lgb_brier:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437f7a0",
   "metadata": {},
   "source": [
    "## 6. Deep Learning Models (Pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671da508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained deep learning model results\n",
    "print(\"Loading deep learning model results...\")\n",
    "\n",
    "with open('../results/metrics/lstm_results.json', 'r') as f:\n",
    "    lstm_results = json.load(f)\n",
    "\n",
    "with open('../results/metrics/transformer_results.json', 'r') as f:\n",
    "    transformer_results = json.load(f)\n",
    "\n",
    "# Rename keys to match\n",
    "lstm_results = {\n",
    "    'accuracy': lstm_results['test_accuracy'],\n",
    "    'roc_auc': lstm_results['test_auc'],\n",
    "    'pr_auc': lstm_results['test_pr_auc'],\n",
    "    'brier': lstm_results['test_brier']\n",
    "}\n",
    "\n",
    "transformer_results = {\n",
    "    'accuracy': transformer_results['test_accuracy'],\n",
    "    'roc_auc': transformer_results['test_auc'],\n",
    "    'pr_auc': transformer_results['test_pr_auc'],\n",
    "    'brier': transformer_results['test_brier']\n",
    "}\n",
    "\n",
    "print(\"âœ… LSTM/GRU Results:\")\n",
    "for k, v in lstm_results.items():\n",
    "    print(f\"   {k}: {v:.3f}\")\n",
    "\n",
    "print(\"\\nâœ… Transformer Results:\")\n",
    "for k, v in transformer_results.items():\n",
    "    print(f\"   {k}: {v:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114878c",
   "metadata": {},
   "source": [
    "## 7. Comprehensive Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfa4c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect ALL model results\n",
    "all_model_results = {\n",
    "    'Logistic Regression': lr_results,\n",
    "    'XGBoost': xgb_results,\n",
    "    'LightGBM': lgb_results,  \n",
    "    'LSTM/GRU': lstm_results,\n",
    "    'Transformer': transformer_results\n",
    "}\n",
    "\n",
    "# Create comparison dataframe\n",
    "comparison_df = pd.DataFrame(all_model_results).T\n",
    "comparison_df.columns = ['Accuracy', 'ROC-AUC', 'PR-AUC', 'Brier Score']\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"COMPLETE MODEL COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(comparison_df.round(4))\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Save to CSV\n",
    "comparison_df.to_csv('../results/metrics/all_models_comparison.csv')\n",
    "print(\"\\nâœ… Saved: ../results/metrics/all_models_comparison.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de2084a2",
   "metadata": {},
   "source": [
    "### 7.1 ROC Curves - All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93c23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves for all models\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Compute ROC curves\n",
    "lr_fpr, lr_tpr, _ = roc_curve(y_test, lr_pred_proba)\n",
    "xgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_pred_proba)\n",
    "lgb_fpr, lgb_tpr, _ = roc_curve(y_test, lgb_pred_proba)\n",
    "\n",
    "# Plot each model\n",
    "ax.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC={lr_auc:.3f})\", linewidth=2)\n",
    "ax.plot(xgb_fpr, xgb_tpr, label=f\"XGBoost (AUC={xgb_auc:.3f})\", linewidth=2)\n",
    "ax.plot(lgb_fpr, lgb_tpr, label=f\"LightGBM (AUC={lgb_auc:.3f})\", linewidth=2)\n",
    "ax.plot([0.5], [0.5], 'o', markersize=10, label=f\"LSTM/GRU (AUC={lstm_results['roc_auc']:.3f})\", alpha=0.7)\n",
    "ax.plot([0.5], [0.5], 's', markersize=10, label=f\"Transformer (AUC={transformer_results['roc_auc']:.3f})\", alpha=0.7)\n",
    "\n",
    "# Diagonal line\n",
    "ax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\n",
    "ax.set_title('ROC Curves - All 5 Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='lower right', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/roc_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… Saved: ../results/figures/roc_curves_all_models.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc894014",
   "metadata": {},
   "source": [
    "### 7.2 Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60201fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PR curves\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "# Plot each model\n",
    "ax.plot(lr_recall, lr_precision, label=f\"Logistic Regression (AUC={lr_pr_auc:.3f})\", linewidth=2)\n",
    "ax.plot(xgb_recall, xgb_precision, label=f\"XGBoost (AUC={xgb_pr_auc:.3f})\", linewidth=2)\n",
    "ax.plot(lgb_recall, lgb_precision, label=f\"LightGBM (AUC={lgb_pr_auc:.3f})\", linewidth=2)\n",
    "\n",
    "# Add deep learning results as markers\n",
    "ax.plot([0.5], [0.5], 'o', markersize=10, label=f\"LSTM/GRU (AUC={lstm_results['pr_auc']:.3f})\", alpha=0.7)\n",
    "ax.plot([0.5], [0.5], 's', markersize=10, label=f\"Transformer (AUC={transformer_results['pr_auc']:.3f})\", alpha=0.7)\n",
    "\n",
    "# Baseline\n",
    "baseline = y_test.mean()\n",
    "ax.axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})', linewidth=1)\n",
    "\n",
    "ax.set_xlabel('Recall', fontsize=12, fontweight='bold')\n",
    "ax.set_ylabel('Precision', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Precision-Recall Curves - All 5 Models', fontsize=14, fontweight='bold')\n",
    "ax.legend(loc='best', fontsize=10)\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/pr_curves_all_models.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… Saved: ../results/figures/pr_curves_all_models.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734c827c",
   "metadata": {},
   "source": [
    "### 7.3 Metrics Comparison Bar Charts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a35598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create metrics comparison visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "metrics = ['Accuracy', 'ROC-AUC', 'PR-AUC', 'Brier Score']\n",
    "colors = ['steelblue', 'coral', 'mediumseagreen', 'plum', 'gold']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    # Sort by metric\n",
    "    sorted_df = comparison_df.sort_values(metric, ascending=(metric == 'Brier Score'))\n",
    "    \n",
    "    bars = ax.barh(sorted_df.index, sorted_df[metric], color=colors[:len(sorted_df)])\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (model_name, value) in enumerate(sorted_df[metric].items()):\n",
    "        ax.text(value + 0.01, i, f'{value:.3f}', va='center', fontweight='bold')\n",
    "    \n",
    "    # Highlight best\n",
    "    best_idx = sorted_df[metric].idxmax() if metric != 'Brier Score' else sorted_df[metric].idxmin()\n",
    "    best_pos = list(sorted_df.index).index(best_idx)\n",
    "    bars[best_pos].set_edgecolor('darkgreen')\n",
    "    bars[best_pos].set_linewidth(3)\n",
    "    \n",
    "    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n",
    "    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.suptitle('Model Performance - All Metrics', fontsize=16, fontweight='bold', y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/figures/metrics_comparison_all.png', dpi=300, bbox_inches='tight')\n",
    "print(\"âœ… Saved: ../results/figures/metrics_comparison_all.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d37504",
   "metadata": {},
   "source": [
    "## 8. Key Findings & Recommendations\n",
    "\n",
    "### Best Models by Metric\n",
    "- **Best ROC-AUC**: {best_roc} \n",
    "- **Best PR-AUC**: {best_pr}\n",
    "- **Best Accuracy**: {best_acc}\n",
    "\n",
    "### Business Recommendations\n",
    "1. **Deploy LSTM/GRU model** for production scoring (best PR-AUC)\n",
    "2. **Use XGBoost SHAP** analysis for explainability to CX team  \n",
    "3. **Target high-risk users** (predicted probability < 0.4) for intervention\n",
    "\n",
    "### All Plots Generated\n",
    "âœ… `../results/figures/xgb_feature_importance.png`\n",
    "âœ… `../results/figures/roc_curves_all_models.png`\n",
    "âœ… `../results/figures/pr_curves_all_models.png`\n",
    "âœ… `../results/figures/metrics_comparison_all.png`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8636196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final summary\n",
    "print(\"=\"*70)\n",
    "print(\"KOLECTO CHURN PREDICTION - ANALYSIS COMPLETE\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nðŸ“Š Models Trained: 5\")\n",
    "print(f\"   1. Logistic Regression\")\n",
    "print(f\"   2. XGBoost\")\n",
    "print(f\"   3. LightGBM\")\n",
    "print(f\"   4. LSTM/GRU\")\n",
    "print(f\"   5. Transformer\")\n",
    "\n",
    "print(f\"\\nðŸ† Best Performance:\")\n",
    "print(f\"   ROC-AUC: {comparison_df['ROC-AUC'].max():.3f} ({comparison_df['ROC-AUC'].idxmax()})\")\n",
    "print(f\"   PR-AUC: {comparison_df['PR-AUC'].max():.3f} ({comparison_df['PR-AUC'].idxmax()})\")\n",
    "\n",
    "print(f\"\\nâœ… All results saved to ../results/\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
