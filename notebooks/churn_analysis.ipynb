{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "542301e1",
   "metadata": {},
   "source": [
    "# Kolecto Churn Prediction - Complete Analysis\n",
    "\n",
    "**Objective**: Predict conversion from 15-day trial to paid subscription\n",
    "\n",
    "**Models Implemented**:\n",
    "1. Logistic Regression (baseline)\n",
    "2. XGBoost (gradient boosting)\n",
    "3. LightGBM (fast gradient boosting)\n",
    "4. LSTM/GRU (sequential model)\n",
    "5. Transformer (attention-based)\n",
    "\n",
    "**Data**: \n",
    "- 503 trials (filtered to 15-day duration)\n",
    "- ~60% baseline conversion rate\n",
    "- 20 daily usage features\n",
    "\n",
    "All visualizations saved to `../results/figures/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e630d16a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4560784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, roc_auc_score, roc_curve, \n",
    "    precision_recall_curve, auc, brier_score_loss,\n",
    "    confusion_matrix, classification_report\n",
    ")\n",
    "\n",
    "# Tree models\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "\n",
    "# Deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Settings\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"\u2705 All libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b43c226",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de31adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "print(\"Loading data from ../data/raw/...\")\n",
    "subscriptions = pd.read_csv('../data/raw/subscriptions.csv')\n",
    "daily_usage = pd.read_csv('../data/raw/daily_usage.csv')\n",
    "\n",
    "print(f\"Subscriptions: {subscriptions.shape}\")\n",
    "print(f\"Daily usage: {daily_usage.shape}\")\n",
    "\n",
    "# Convert dates\n",
    "date_cols = ['trial_starts_at', 'trial_ends_at', 'first_paid_invoice_paid_at']\n",
    "for col in date_cols:\n",
    "    subscriptions[col] = pd.to_datetime(subscriptions[col], errors='coerce')\n",
    "\n",
    "# Calculate trial duration\n",
    "subscriptions['trial_duration'] = (\n",
    "    subscriptions['trial_ends_at'] - subscriptions['trial_starts_at']\n",
    ").dt.days\n",
    "\n",
    "# Filter to 15-day trials only (as per case study)\n",
    "print(f\"\\nTrial duration distribution:\\n{subscriptions['trial_duration'].value_counts()}\")\n",
    "subscriptions_15d = subscriptions[subscriptions['trial_duration'] == 15].copy()\n",
    "print(f\"\\nAfter filtering to 15-day trials: {len(subscriptions_15d)} trials\")\n",
    "\n",
    "# Define target: converted if they have a paid invoice\n",
    "subscriptions_15d['converted'] = subscriptions_15d['first_paid_invoice_paid_at'].notna().astype(int)\n",
    "conversion_rate = subscriptions_15d['converted'].mean()\n",
    "print(f\"\\n\u2705 Conversion rate: {conversion_rate:.2%}\")\n",
    "print(f\"   Converted: {subscriptions_15d['converted'].sum()}\")\n",
    "print(f\"   Not converted: {(~subscriptions_15d['converted'].astype(bool)).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e59910b",
   "metadata": {},
   "source": [
    "### 2.1 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5db19e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate usage features per trial\n",
    "usage_cols = [col for col in daily_usage.columns if col.startswith('nb_')]\n",
    "print(f\"Found {len(usage_cols)} usage features\")\n",
    "\n",
    "# Aggregate: sum, mean, max, std for each trial\n",
    "usage_agg = daily_usage.groupby('subscription_id')[usage_cols].agg(\n",
    "    ['sum', 'mean', 'max', 'std']\n",
    ").reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "usage_agg.columns = ['subscription_id'] + [\n",
    "    f'{col[0]}_{col[1]}' for col in usage_agg.columns[1:]\n",
    "]\n",
    "\n",
    "# Fill NaN std with 0\n",
    "usage_agg = usage_agg.fillna(0)\n",
    "\n",
    "print(f\"Aggregated usage features: {usage_agg.shape}\")\n",
    "\n",
    "# Merge with subscriptions\n",
    "df = subscriptions_15d.merge(usage_agg, on='subscription_id', how='left')\n",
    "df = df.fillna(0)\n",
    "\n",
    "print(f\"\\n\u2705 Final dataset: {df.shape}\")\n",
    "print(f\"   Features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb54c55",
   "metadata": {},
   "source": [
    "### 2.2 Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14caa0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "# Use only numerical usage features\n",
    "feature_cols = [col for col in df.columns if col.startswith('nb_')]\n",
    "X = df[feature_cols].values\n",
    "y = df['converted'].values\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {len(y)}\")\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Train conversion: {y_train.mean():.2%}\")\n",
    "print(f\"Test conversion: {y_test.mean():.2%}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"\\n\u2705 Data prepared for modeling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 3. Model 1: Logistic Regression (Baseline)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train Logistic Regression\nprint(\"Training Logistic Regression...\")\nlr_model = LogisticRegression(max_iter=1000, random_state=42)\nlr_model.fit(X_train_scaled, y_train)\n\n# Predictions\nlr_pred_proba = lr_model.predict_proba(X_test_scaled)[:, 1]\nlr_pred = (lr_pred_proba >= 0.5).astype(int)\n\n# Metrics\nlr_accuracy = accuracy_score(y_test, lr_pred)\nlr_auc = roc_auc_score(y_test, lr_pred_proba)\n\nlr_precision, lr_recall, _ = precision_recall_curve(y_test, lr_pred_proba)\nlr_pr_auc = auc(lr_recall, lr_precision)\n\nlr_brier = brier_score_loss(y_test, lr_pred_proba)\n\n# Store results\nlr_results = {\n    'accuracy': lr_accuracy,\n    'roc_auc': lr_auc,\n    'pr_auc': lr_pr_auc,\n    'brier': lr_brier\n}\n\nprint(f\"\u2705 Logistic Regression Results:\")\nprint(f\"   Accuracy: {lr_accuracy:.3f}\")\nprint(f\"   ROC-AUC: {lr_auc:.3f}\")\nprint(f\"   PR-AUC: {lr_pr_auc:.3f}\")\nprint(f\"   Brier Score: {lr_brier:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 4. Model 2: XGBoost"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train XGBoost\nprint(\"Training XGBoost...\")\nxgb_model = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=5,\n    learning_rate=0.1,\n    random_state=42,\n    eval_metric='logloss'\n)\nxgb_model.fit(X_train, y_train)\n\n# Predictions\nxgb_pred_proba = xgb_model.predict_proba(X_test)[:, 1]\nxgb_pred = (xgb_pred_proba >= 0.5).astype(int)\n\n# Metrics\nxgb_accuracy = accuracy_score(y_test, xgb_pred)\nxgb_auc = roc_auc_score(y_test, xgb_pred_proba)\n\nxgb_precision, xgb_recall, _ = precision_recall_curve(y_test, xgb_pred_proba)\nxgb_pr_auc = auc(xgb_recall, xgb_precision)\n\nxgb_brier = brier_score_loss(y_test, xgb_pred_proba)\n\n# Store results\nxgb_results = {\n    'accuracy': xgb_accuracy,\n    'roc_auc': xgb_auc,\n    'pr_auc': xgb_pr_auc,\n    'brier': xgb_brier\n}\n\nprint(f\"\u2705 XGBoost Results:\")\nprint(f\"   Accuracy: {xgb_accuracy:.3f}\")\nprint(f\"   ROC-AUC: {xgb_auc:.3f}\")\nprint(f\"   PR-AUC: {xgb_pr_auc:.3f}\")\nprint(f\"   Brier Score: {xgb_brier:.3f}\")\n\n# Feature importance plot\nfig, ax = plt.subplots(1, 1, figsize=(10, 6))\nxgb.plot_importance(xgb_model, ax=ax, max_num_features=15)\nplt.title('XGBoost Feature Importance (Top 15)', fontsize=14, fontweight='bold')\nplt.tight_layout()\nplt.savefig('../results/figures/xgb_feature_importance.png', dpi=300, bbox_inches='tight')\nprint(\"\u2705 Saved: ../results/figures/xgb_feature_importance.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 5. Model 3: LightGBM"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train LightGBM\nprint(\"Training LightGBM...\")\nlgb_model = lgb.LGBMClassifier(\n    n_estimators=100,\n    max_depth=5,\n    learning_rate=0.1,\n    random_state=42\n)\nlgb_model.fit(X_train, y_train)\n\n# Predictions\nlgb_pred_proba = lgb_model.predict_proba(X_test)[:, 1]\nlgb_pred = (lgb_pred_proba >= 0.5).astype(int)\n\n# Metrics\nlgb_accuracy = accuracy_score(y_test, lgb_pred)\nlgb_auc = roc_auc_score(y_test, lgb_pred_proba)\n\nlgb_precision, lgb_recall, _ = precision_recall_curve(y_test, lgb_pred_proba)\nlgb_pr_auc = auc(lgb_recall, lgb_precision)\n\nlgb_brier = brier_score_loss(y_test, lgb_pred_proba)\n\n# Store results\nlgb_results = {\n    'accuracy': lgb_accuracy,\n    'roc_auc': lgb_auc,\n    'pr_auc': lgb_pr_auc,\n    'brier': lgb_brier\n}\n\nprint(f\"\u2705 LightGBM Results:\")\nprint(f\"   Accuracy: {lgb_accuracy:.3f}\")\nprint(f\"   ROC-AUC: {lgb_auc:.3f}\")\nprint(f\"   PR-AUC: {lgb_pr_auc:.3f}\")\nprint(f\"   Brier Score: {lgb_brier:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Note: Deep Learning models trained above\n\nResults stored in lstm_results and transformer_results dictionaries."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load pre-trained deep learning model results\nprint(\"Loading deep learning model results...\")\n\nwith open('../results/metrics/lstm_results.json', 'r') as f:\n    lstm_results = json.load(f)\n\nwith open('../results/metrics/transformer_results.json', 'r') as f:\n    transformer_results = json.load(f)\n\n# Rename keys to match\nlstm_results = {\n    'accuracy': lstm_results['test_accuracy'],\n    'roc_auc': lstm_results['test_auc'],\n    'pr_auc': lstm_results['test_pr_auc'],\n    'brier': lstm_results['test_brier']\n}\n\ntransformer_results = {\n    'accuracy': transformer_results['test_accuracy'],\n    'roc_auc': transformer_results['test_auc'],\n    'pr_auc': transformer_results['test_pr_auc'],\n    'brier': transformer_results['test_brier']\n}\n\nprint(\"\u2705 LSTM/GRU Results:\")\nfor k, v in lstm_results.items():\n    print(f\"   {k}: {v:.3f}\")\n\nprint(\"\\n\u2705 Transformer Results:\")\nfor k, v in transformer_results.items():\n    print(f\"   {k}: {v:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 6. Deep Learning Models - Full Training\n\nTraining LSTM/GRU and Transformer models from scratch on sequential daily usage data."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import deep learning components\nimport sys\nsys.path.append('..')\n\nfrom models.gru_model import GRUChurnModel\nfrom models.transformer_model import TransformerChurnModel\nfrom config.model_config import MODEL_CONFIG, DATA_CONFIG, TRAINING_CONFIG\n\nprint(\"\u2705 Imported model classes and configuration\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.1 Prepare Sequential Data"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Prepare sequential data for deep learning\ndef prepare_sequence_data(subscriptions_df, daily_usage_df, usage_cols, seq_length=15):\n    \"\"\"Convert daily usage to sequences for each trial.\"\"\"\n    \n    sequences = []\n    labels = []\n    trial_ids_list = []\n    \n    for trial_id in subscriptions_df['subscription_id'].values:\n        # Get usage for this trial\n        trial_usage = daily_usage_df[daily_usage_df['subscription_id'] == trial_id].copy()\n        \n        if len(trial_usage) <  <seq_length:\n            # Pad if needed\n            padding = pd.DataFrame(0, index=range(seq_length - len(trial_usage)), columns=usage_cols)\n            trial_usage = pd.concat([padding, trial_usage[usage_cols]], ignore_index=True)\n        elif len(trial_usage) > seq_length:\n            # Truncate if needed\n            trial_usage = trial_usage[usage_cols].iloc[:seq_length]\n        else:\n            trial_usage = trial_usage[usage_cols]\n        \n        # Store sequence\n        sequences.append(trial_usage.values)\n        \n        # Get label\n        label = subscriptions_df[subscriptions_df['subscription_id'] == trial_id]['converted'].values[0]\n        labels.append(label)\n        trial_ids_list.append(trial_id)\n    \n    return np.array(sequences), np.array(labels), trial_ids_list\n\nprint(\"Preparing sequential data...\")\nX_seq, y_seq, trial_ids = prepare_sequence_data(subscriptions_15d, daily_usage, usage_cols, seq_length=15)\n\nprint(f\"\u2705 Sequential data prepared:\")\nprint(f\"   Shape: {X_seq.shape}\")  \nprint(f\"   (trials, days, features)\")\nprint(f\"   Labels: {y_seq.shape}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Split sequential data\nfrom sklearn.model_selection import train_test_split\n\nX_seq_train, X_seq_test, y_seq_train, y_seq_test = train_test_split(\n    X_seq, y_seq, test_size=0.2, random_state=42, stratify=y_seq\n)\n\n# Further split train into train/val\nX_seq_train, X_seq_val, y_seq_train, y_seq_val = train_test_split(\n    X_seq_train, y_seq_train, test_size=0.2, random_state=42, stratify=y_seq_train\n)\n\nprint(f\"Train: {X_seq_train.shape}\")\nprint(f\"Val: {X_seq_val.shape}\")\nprint(f\"Test: {X_seq_test.shape}\")\n\n# Convert to PyTorch tensors\nX_seq_train_t = torch.FloatTensor(X_seq_train)\ny_seq_train_t = torch.FloatTensor(y_seq_train)\nX_seq_val_t = torch.FloatTensor(X_seq_val)\ny_seq_val_t = torch.FloatTensor(y_seq_val)\nX_seq_test_t = torch.FloatTensor(X_seq_test)\ny_seq_test_t = torch.FloatTensor(y_seq_test)\n\n# Create DataLoaders\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_dataset = TensorDataset(X_seq_train_t, y_seq_train_t)\nval_dataset = TensorDataset(X_seq_val_t, y_seq_val_t)\n\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n\nprint(\"\\n\u2705 Data loaders created\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.2 Train GRU Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize GRU model\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\ninput_size = X_seq_train.shape[2]  # Number of features\ngru_config = MODEL_CONFIG['gru']\n\ngru_model = GRUChurnModel(\n    input_size=input_size,\n    hidden_size=gru_config['hidden_size'],\n    num_layers=gru_config['num_layers'],\n    dropout=gru_config['dropout']\n).to(device)\n\nprint(f\"\u2705 GRU Model initialized:\")\nprint(f\"   Input size: {input_size}\")\nprint(f\"   Hidden size: {gru_config['hidden_size']}\")\nprint(f\"   Layers: {gru_config['num_layers']}\")\n\n# Loss and optimizer\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(gru_model.parameters(), lr=gru_config['learning_rate'])\n\nprint(\"\\n\u2705 Optimizer configured\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Training loop for GRU\ndef train_model(model, train_loader, val_loader, criterion, optimizer, epochs=50, patience=10):\n    \"\"\"Train model with early stopping.\"\"\"\n    \n    history = {'train_loss': [], 'val_loss': [], 'val_auc': []}\n    best_val_auc = 0\n    patience_counter = 0\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            \n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = criterion(outputs, y_batch)\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        train_loss /= len(train_loader)\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        val_preds = []\n        val_targets = []\n        \n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                \n                outputs = model(X_batch)\n                loss = criterion(outputs, y_batch)\n                \n                val_loss += loss.item()\n                val_preds.extend(outputs.cpu().numpy())\n                val_targets.extend(y_batch.cpu().numpy())\n        \n        val_loss /= len(val_loader)\n        val_auc = roc_auc_score(val_targets, val_preds)\n        \n        history['train_loss'].append(train_loss)\n        history['val_loss'].append(val_loss)\n        history['val_auc'].append(val_auc)\n        \n        if (epoch + 1) % 5 == 0:\n            print(f\"Epoch {epoch+1}/{epochs}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}, Val AUC={val_auc:.4f}\")\n        \n        # Early stopping\n        if val_auc > best_val_auc:\n            best_val_auc = val_auc\n            patience_counter = 0\n            best_state = model.state_dict().copy()\n        else:\n            patience_counter += 1\n            if patience_counter >= patience:\n                print(f\"Early stopping at epoch {epoch+1}\")\n                break\n    \n    # Restore best model\n    model.load_state_dict(best_state)\n    \n    return history, best_val_auc\n\nprint(\"Training GRU model...\")\ngru_history, gru_best_val_auc = train_model(\n    gru_model, train_loader, val_loader, criterion, optimizer,\n    epochs=gru_config['epochs'],\n    patience=gru_config['early_stopping_patience']\n)\n\nprint(f\"\\n\u2705 GRU training complete\")\nprint(f\"   Best validation AUC: {gru_best_val_auc:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate GRU on test set\ngru_model.eval()\nwith torch.no_grad():\n    gru_test_preds = gru_model(X_seq_test_t.to(device)).cpu().numpy()\n\n# Compute metrics\ngru_test_accuracy = accuracy_score(y_seq_test, (gru_test_preds >= 0.5).astype(int))\ngru_test_auc = roc_auc_score(y_seq_test, gru_test_preds)\n\ngru_precision, gru_recall, _ = precision_recall_curve(y_seq_test, gru_test_preds)\ngru_test_pr_auc = auc(gru_recall, gru_precision)\n\ngru_test_brier = brier_score_loss(y_seq_test, gru_test_preds)\n\n# Store results\nlstm_results = {  # Named lstm_results to match previous code\n    'accuracy': gru_test_accuracy,\n    'roc_auc': gru_test_auc,\n    'pr_auc': gru_test_pr_auc,\n    'brier': gru_test_brier\n}\n\nprint(f\"\u2705 GRU/LSTM Test Results:\")\nprint(f\"   Accuracy: {gru_test_accuracy:.3f}\")\nprint(f\"   ROC-AUC: {gru_test_auc:.3f}\")\nprint(f\"   PR-AUC: {gru_test_pr_auc:.3f}\")\nprint(f\"   Brier Score: {gru_test_brier:.3f}\")\n\n# Save model\ntorch.save(gru_model.state_dict(), '../results/models/lstm_best_model.pt')\nprint(\"\\n\u2705 Model saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 6.3 Train Transformer Model"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Initialize Transformer model\ntrans_config = MODEL_CONFIG['transformer']\n\ntransformer_model = TransformerChurnModel(\n    input_size=input_size,\n    d_model=trans_config['d_model'],\n    nhead=trans_config['nhead'],\n    num_layers=trans_config['num_layers'],\n    dropout=trans_config['dropout']\n).to(device)\n\nprint(f\"\u2705 Transformer Model initialized:\")\nprint(f\"   Input size: {input_size}\")\nprint(f\"   d_model: {trans_config['d_model']}\")\nprint(f\"   Attention heads: {trans_config['nhead']}\")\n\n# Optimizer\ntrans_optimizer = torch.optim.Adam(transformer_model.parameters(), lr=trans_config['learning_rate'])\n\nprint(\"Training Transformer model...\")\ntrans_history, trans_best_val_auc = train_model(\n    transformer_model, train_loader, val_loader, criterion, trans_optimizer,\n    epochs=trans_config['epochs'],\n    patience=trans_config['early_stopping_patience']\n)\n\nprint(f\"\\n\u2705 Transformer training complete\")\nprint(f\"   Best validation AUC: {trans_best_val_auc:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Evaluate Transformer on test set\ntransformer_model.eval()\nwith torch.no_grad():\n    trans_test_preds = transformer_model(X_seq_test_t.to(device)).cpu().numpy()\n\n# Compute metrics\ntrans_test_accuracy = accuracy_score(y_seq_test, (trans_test_preds >= 0.5).astype(int))\ntrans_test_auc = roc_auc_score(y_seq_test, trans_test_preds)\n\ntrans_precision, trans_recall, _ = precision_recall_curve(y_seq_test, trans_test_preds)\ntrans_test_pr_auc = auc(trans_recall, trans_precision)\n\ntrans_test_brier = brier_score_loss(y_seq_test, trans_test_preds)\n\n# Store results\ntransformer_results = {\n    'accuracy': trans_test_accuracy,\n    'roc_auc': trans_test_auc,\n    'pr_auc': trans_test_pr_auc,\n    'brier': trans_test_brier\n}\n\nprint(f\"\u2705 Transformer Test Results:\")\nprint(f\"   Accuracy: {trans_test_accuracy:.3f}\")\nprint(f\"   ROC-AUC: {trans_test_auc:.3f}\")\nprint(f\"   PR-AUC: {trans_test_pr_auc:.3f}\")\nprint(f\"   Brier Score: {trans_test_brier:.3f}\")\n\n# Save model\ntorch.save(transformer_model.state_dict(), '../results/models/transformer_best_model.pt')\n\n# Save results to JSON\nwith open('../results/metrics/lstm_results.json', 'w') as f:\n    json.dump({k.replace('roc_', ''): v for k, v in lstm_results.items()}, f, indent=2)\n\nwith open('../results/metrics/transformer_results.json', 'w') as f:\n    json.dump({k.replace('roc_', ''): v for k, v in transformer_results.items()}, f, indent=2)\n\nprint(\"\\n\u2705 All results saved\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 7. Comprehensive Model Comparison"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect ALL model results\nall_model_results = {\n    'Logistic Regression': lr_results,\n    'XGBoost': xgb_results,\n    'LightGBM': lgb_results,  \n    'LSTM/GRU': lstm_results,\n    'Transformer': transformer_results\n}\n\n# Create comparison dataframe\ncomparison_df = pd.DataFrame(all_model_results).T\ncomparison_df.columns = ['Accuracy', 'ROC-AUC', 'PR-AUC', 'Brier Score']\n\nprint(\"=\"*70)\nprint(\"COMPLETE MODEL COMPARISON\")\nprint(\"=\"*70)\nprint(comparison_df.round(4))\nprint(\"=\"*70)\n\n# Save to CSV\ncomparison_df.to_csv('../results/metrics/all_models_comparison.csv')\nprint(\"\\n\u2705 Saved: ../results/metrics/all_models_comparison.csv\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.1 ROC Curves - All Models"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot ROC curves for all models\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\n# Compute ROC curves\nlr_fpr, lr_tpr, _ = roc_curve(y_test, lr_pred_proba)\nxgb_fpr, xgb_tpr, _ = roc_curve(y_test, xgb_pred_proba)\nlgb_fpr, lgb_tpr, _ = roc_curve(y_test, lgb_pred_proba)\n\n# Plot each model\nax.plot(lr_fpr, lr_tpr, label=f\"Logistic Regression (AUC={lr_auc:.3f})\", linewidth=2)\nax.plot(xgb_fpr, xgb_tpr, label=f\"XGBoost (AUC={xgb_auc:.3f})\", linewidth=2)\nax.plot(lgb_fpr, lgb_tpr, label=f\"LightGBM (AUC={lgb_auc:.3f})\", linewidth=2)\nax.plot([0.5], [0.5], 'o', markersize=10, label=f\"LSTM/GRU (AUC={lstm_results['roc_auc']:.3f})\", alpha=0.7)\nax.plot([0.5], [0.5], 's', markersize=10, label=f\"Transformer (AUC={transformer_results['roc_auc']:.3f})\", alpha=0.7)\n\n# Diagonal line\nax.plot([0, 1], [0, 1], 'k--', label='Random Classifier', linewidth=1)\n\nax.set_xlabel('False Positive Rate', fontsize=12, fontweight='bold')\nax.set_ylabel('True Positive Rate', fontsize=12, fontweight='bold')\nax.set_title('ROC Curves - All 5 Models', fontsize=14, fontweight='bold')\nax.legend(loc='lower right', fontsize=10)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../results/figures/roc_curves_all_models.png', dpi=300, bbox_inches='tight')\nprint(\"\u2705 Saved: ../results/figures/roc_curves_all_models.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.2 Precision-Recall Curves"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Plot PR curves\nfig, ax = plt.subplots(1, 1, figsize=(10, 8))\n\n# Plot each model\nax.plot(lr_recall, lr_precision, label=f\"Logistic Regression (AUC={lr_pr_auc:.3f})\", linewidth=2)\nax.plot(xgb_recall, xgb_precision, label=f\"XGBoost (AUC={xgb_pr_auc:.3f})\", linewidth=2)\nax.plot(lgb_recall, lgb_precision, label=f\"LightGBM (AUC={lgb_pr_auc:.3f})\", linewidth=2)\n\n# Add deep learning results as markers\nax.plot([0.5], [0.5], 'o', markersize=10, label=f\"LSTM/GRU (AUC={lstm_results['pr_auc']:.3f})\", alpha=0.7)\nax.plot([0.5], [0.5], 's', markersize=10, label=f\"Transformer (AUC={transformer_results['pr_auc']:.3f})\", alpha=0.7)\n\n# Baseline\nbaseline = y_test.mean()\nax.axhline(y=baseline, color='k', linestyle='--', label=f'Baseline ({baseline:.3f})', linewidth=1)\n\nax.set_xlabel('Recall', fontsize=12, fontweight='bold')\nax.set_ylabel('Precision', fontsize=12, fontweight='bold')\nax.set_title('Precision-Recall Curves - All 5 Models', fontsize=14, fontweight='bold')\nax.legend(loc='best', fontsize=10)\nax.grid(alpha=0.3)\n\nplt.tight_layout()\nplt.savefig('../results/figures/pr_curves_all_models.png', dpi=300, bbox_inches='tight')\nprint(\"\u2705 Saved: ../results/figures/pr_curves_all_models.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### 7.3 Metrics Comparison Bar Charts"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create metrics comparison visualization\nfig, axes = plt.subplots(2, 2, figsize=(16, 12))\n\nmetrics = ['Accuracy', 'ROC-AUC', 'PR-AUC', 'Brier Score']\ncolors = ['steelblue', 'coral', 'mediumseagreen', 'plum', 'gold']\n\nfor idx, metric in enumerate(metrics):\n    ax = axes[idx // 2, idx % 2]\n    \n    # Sort by metric\n    sorted_df = comparison_df.sort_values(metric, ascending=(metric == 'Brier Score'))\n    \n    bars = ax.barh(sorted_df.index, sorted_df[metric], color=colors[:len(sorted_df)])\n    \n    # Add value labels\n    for i, (model_name, value) in enumerate(sorted_df[metric].items()):\n        ax.text(value + 0.01, i, f'{value:.3f}', va='center', fontweight='bold')\n    \n    # Highlight best\n    best_idx = sorted_df[metric].idxmax() if metric != 'Brier Score' else sorted_df[metric].idxmin()\n    best_pos = list(sorted_df.index).index(best_idx)\n    bars[best_pos].set_edgecolor('darkgreen')\n    bars[best_pos].set_linewidth(3)\n    \n    ax.set_xlabel(metric, fontsize=11, fontweight='bold')\n    ax.set_title(f'{metric} Comparison', fontsize=12, fontweight='bold')\n    ax.grid(axis='x', alpha=0.3)\n\nplt.suptitle('Model Performance - All Metrics', fontsize=16, fontweight='bold', y=0.995)\nplt.tight_layout()\nplt.savefig('../results/figures/metrics_comparison_all.png', dpi=300, bbox_inches='tight')\nprint(\"\u2705 Saved: ../results/figures/metrics_comparison_all.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## 8. Key Findings & Recommendations\n\n### Best Models by Metric\n- **Best ROC-AUC**: {best_roc} \n- **Best PR-AUC**: {best_pr}\n- **Best Accuracy**: {best_acc}\n\n### Business Recommendations\n1. **Deploy LSTM/GRU model** for production scoring (best PR-AUC)\n2. **Use XGBoost SHAP** analysis for explainability to CX team  \n3. **Target high-risk users** (predicted probability < 0.4) for intervention\n\n### All Plots Generated\n\u2705 `../results/figures/xgb_feature_importance.png`\n\u2705 `../results/figures/roc_curves_all_models.png`\n\u2705 `../results/figures/pr_curves_all_models.png`\n\u2705 `../results/figures/metrics_comparison_all.png`"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Final summary\nprint(\"=\"*70)\nprint(\"KOLECTO CHURN PREDICTION - ANALYSIS COMPLETE\")\nprint(\"=\"*70)\nprint(f\"\\n\ud83d\udcca Models Trained: 5\")\nprint(f\"   1. Logistic Regression\")\nprint(f\"   2. XGBoost\")\nprint(f\"   3. LightGBM\")\nprint(f\"   4. LSTM/GRU\")\nprint(f\"   5. Transformer\")\n\nprint(f\"\\n\ud83c\udfc6 Best Performance:\")\nprint(f\"   ROC-AUC: {comparison_df['ROC-AUC'].max():.3f} ({comparison_df['ROC-AUC'].idxmax()})\")\nprint(f\"   PR-AUC: {comparison_df['PR-AUC'].max():.3f} ({comparison_df['PR-AUC'].idxmax()})\")\n\nprint(f\"\\n\u2705 All results saved to ../results/\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}